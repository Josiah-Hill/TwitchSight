{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d46ca64",
   "metadata": {},
   "source": [
    "# Web Scraping: Twitch UserVoice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab991de",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c56b10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0846e1c",
   "metadata": {},
   "source": [
    "## Final Scraper for \"Chat\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0489f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "# Target URL\n",
    "base_url = \"https://twitch.uservoice.com/forums/310201-chat?filter=top&page=\"\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    #if page_no > 15:# Stop after 15 pages\n",
    "    #    break\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310201-chat/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Chat'\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76079b58",
   "metadata": {},
   "source": [
    "==========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c31ec",
   "metadata": {},
   "source": [
    "## Scraper for other categories\n",
    "* Account Management - [X]\n",
    "* Ads - [x]\n",
    "* Badges/Emotes - [x]\n",
    "* Bits - [x]\n",
    "* Channel Page - [x]\n",
    "* Channel Points - [x]\n",
    "* Charity - [x]\n",
    "* Chat - [x]\n",
    "* Creator Camp - [x]\n",
    "* Creator Dashboard - [x]\n",
    "* Creator Dashboard: Stream Manager - [x]\n",
    "* Creators and Stream Features - [x]\n",
    "* Customer Experience - [x]\n",
    "* Developers - [x]\n",
    "* Discover - [x]\n",
    "* Extensions - [x]\n",
    "* IGDB - [x]\n",
    "* IRL Events and Merch - [x]\n",
    "* Localization - [x]\n",
    "* Moderation - [x]\n",
    "* Purchase Management - [x]\n",
    "* Safety - [x]\n",
    "* Subscriptions - [x]\n",
    "* Twitch Applications: Consoles - [x]\n",
    "* Twitch Applications: Mobile - [x]\n",
    "* Twitch Applications: TV Apps - [x]\n",
    "* Twitch Studio - [x]\n",
    "* User Accessibility - []\n",
    "* Video Features - []\n",
    "* Video Performance - []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f9bbb6",
   "metadata": {},
   "source": [
    "## Account Manegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ede81d34",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m num_comments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_comments)\n\u001b[0;32m     53\u001b[0m t_tag \u001b[38;5;241m=\u001b[39m meta_div\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdeas similar to\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 54\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[43mt_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Initializing category for each scrape\u001b[39;00m\n\u001b[0;32m     57\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccount Management\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m##Change this for other cat ##\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310228-account-management?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310228-account-management/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Account Management' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            if link == 'https://twitch.uservoice.com/forums/310228-account-management/suggestions/46216468-phone-number-issue':\n",
    "                author = np.nan\n",
    "                date = np.nan\n",
    "                continue\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfc6bf",
   "metadata": {},
   "source": [
    "## Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7cfbf5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310237-ads?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310237-ads/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Ads' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281811c6",
   "metadata": {},
   "source": [
    "## Badges/Emotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c922c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/928738-badges-emotes?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/928738-badges-emotes/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Badges/Emotes' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9073d",
   "metadata": {},
   "source": [
    "## Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5451497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/921826-bits?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/921826-bits/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Bits' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e12b8",
   "metadata": {},
   "source": [
    "## Channel Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4279c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/928429-channel-page/suggestions/40008394-donation-url-in-social-links: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/928429-channel-page/suggestions/40008394-donation-url-in-social-links (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F4659B5A30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/928429-channel-page/suggestions/40379764-custom-links-should-use-site-favicons-not-a-hyper: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/928429-channel-page/suggestions/40379764-custom-links-should-use-site-favicons-not-a-hyper (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F4659B5A00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/928429-channel-page/suggestions/40005379-discord-logo-for-links-on-the-new-channel-page: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/928429-channel-page/suggestions/40005379-discord-logo-for-links-on-the-new-channel-page (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F4659B5A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/928429-channel-page?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/928429-channel-page/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Channel Page' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b64cc",
   "metadata": {},
   "source": [
    "## Channel Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98da2418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/932221-channel-points?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/932221-channel-points/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Channel Points' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f80b6",
   "metadata": {},
   "source": [
    "## Charity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89d978ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/945934-charity?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/945934-charity/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Charity' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fca8d",
   "metadata": {},
   "source": [
    "## Creator Camp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eae6f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/926239-creator-camp?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/926239-creator-camp/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Creator Camp' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1ab33",
   "metadata": {},
   "source": [
    "## Creator Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6f4b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/924712-creator-dashboard?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/924712-creator-dashboard/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Creator Dashboard' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1a4ee",
   "metadata": {},
   "source": [
    "## Creator Dashboard: Stream Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "23b0c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/933484-creator-dashboard-stream-manager?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/933484-creator-dashboard-stream-manager/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Creator Dashboard: Stream Manager' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce3f90",
   "metadata": {},
   "source": [
    "## Creators and Stream Features [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b1d78325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/923383-creators-and-stream-features?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/923383-creators-and-stream-features/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Creators and Stream Features' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d985520",
   "metadata": {},
   "source": [
    "## Customer Experience [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ffdda62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/934332-customer-experience/suggestions/46087618-stop-merging-ideas-without-first-confirming-with-t: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/934332-customer-experience/suggestions/46087618-stop-merging-ideas-without-first-confirming-with-t (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52281EE20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/934332-customer-experience/suggestions/46087618-stop-merging-ideas-without-first-confirming-with-t: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/934332-customer-experience/suggestions/46087618-stop-merging-ideas-without-first-confirming-with-t (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52281EDF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/934332-customer-experience?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/934332-customer-experience/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Customer Experience' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114bcae",
   "metadata": {},
   "source": [
    "## Developers [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2281c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42012043-add-is-prime-to-subscription-events: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42012043-add-is-prime-to-subscription-events (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/43241439-channel-subscriptions-to-include-who-gave-the-gift: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/43241439-channel-subscriptions-to-include-who-gave-the-gift (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DB20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42454273-subscriber-upgrades (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42008110-please-provide-an-event-sub-that-emits-resub-paym (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DAC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42012043-add-is-prime-to-subscription-events: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42012043-add-is-prime-to-subscription-events (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52A83DA90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39228784-extend-clips-api-to-provide-the-mp4-url-so-editors: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39228784-extend-clips-api-to-provide-the-mp4-url-so-editors (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52BA4E370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/37563622-live-activated-channels-w-extension-webhook: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/37563622-live-activated-channels-w-extension-webhook (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52C90F730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41272606-extend-start-commercial-error-message-to-return-r: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41272606-extend-start-commercial-error-message-to-return-r (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52DCB50D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41272606-extend-start-commercial-error-message-to-return-r: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41272606-extend-start-commercial-error-message-to-return-r (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52DCB80D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39547780-emotes-by-channelid: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39547780-emotes-by-channelid (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52F7AE1C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52F7AE190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39547780-emotes-by-channelid: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39547780-emotes-by-channelid (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52F7AE1F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52F7B11C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42008104-please-provide-an-event-sub-that-emits-sub-notifi: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42008104-please-provide-an-event-sub-that-emits-sub-notifi (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F52F8625B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39622321-rerun-api: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39622321-rerun-api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F532DAEF10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/38375011-mature-flag-on-get-streams-endpoint: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/38375011-mature-flag-on-get-streams-endpoint (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F534AB3B20>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/40871443-pubsub-cheer-emote-placement: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/40871443-pubsub-cheer-emote-placement (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F534B65640>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41137957-upload-emotes (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5386E1160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/41016568-context-resubgift-in-resub-events?tracking_code=8f96eb083a992136c313fb0ca1d79054: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/41016568-context-resubgift-in-resub-events?tracking_code=8f96eb083a992136c313fb0ca1d79054 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5390330D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39645769-moderator-actions-pubsub-topic-should-work-for-all: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39645769-moderator-actions-pubsub-topic-should-work-for-all (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F53C54A070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/37563241-expose-vips-in-the-api: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/37563241-expose-vips-in-the-api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F53DD5CD00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/42991875-please-provide-a-topic-s-for-all-kinds-of-moderat: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/42991875-please-provide-a-topic-s-for-all-kinds-of-moderat (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F53EF6C9A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/44500245-revise-the-channel-subscription-topics: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/44500245-revise-the-channel-subscription-topics (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F544368280>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/43813413-provide-a-gift-event-id-in-the-channel-subscribe (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F544368250>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/45445489-please-provide-an-api-to-manage-donations: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/45445489-please-provide-an-api-to-manage-donations (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5454E2130>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310213-developers/suggestions/39910141-copy-subscription-support-for-channel-moderation-s: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310213-developers/suggestions/39910141-copy-subscription-support-for-channel-moderation-s (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F54A5732E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310213-developers?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310213-developers/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Developers' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423f4d6",
   "metadata": {},
   "source": [
    "## Discover [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "31757de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310210-discover/suggestions/47257001-channels-marked-not-interested-not-syncing-acros: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310210-discover/suggestions/47257001-channels-marked-not-interested-not-syncing-acros (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5823D13A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310210-discover/suggestions/44079120-create-a-religions-faith-and-beliefs-category: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310210-discover/suggestions/44079120-create-a-religions-faith-and-beliefs-category (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F59ABF3100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310210-discover/suggestions/44079120-create-a-religions-faith-and-beliefs-category: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310210-discover/suggestions/44079120-create-a-religions-faith-and-beliefs-category (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F59ABF5100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310210-discover?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310210-discover/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Discover' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdba24",
   "metadata": {},
   "source": [
    "## Extensions [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "960dbdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/904711-extensions?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/904711-extensions/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Extensions' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160cfa60",
   "metadata": {},
   "source": [
    "## IGDB [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9fbf454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/929953-igdb?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/929953-igdb/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'IGDB' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e28ed",
   "metadata": {},
   "source": [
    "## IRL Events and Merch [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c14b47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/924751-irl-events-and-merch?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/924751-irl-events-and-merch/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'IRL Events and Merch' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c792408f",
   "metadata": {},
   "source": [
    "## Localization [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6c6a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/924619-localization/suggestions/43430019-add-ukrainian-localization-please: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/924619-localization/suggestions/43430019-add-ukrainian-localization-please (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5C5618190>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/924619-localization/suggestions/43430019-add-ukrainian-localization-please: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/924619-localization/suggestions/43430019-add-ukrainian-localization-please (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5C5618160>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/924619-localization?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/924619-localization/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Localization' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282096ca",
   "metadata": {},
   "source": [
    "## Moderation [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "745f3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/951706-moderation/suggestions/42550657-mobile-moderation: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/951706-moderation/suggestions/42550657-mobile-moderation (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5DA661700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/951706-moderation/suggestions/42550657-mobile-moderation: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/951706-moderation/suggestions/42550657-mobile-moderation (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5DA6616D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/951706-moderation?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/951706-moderation/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Moderation' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da515c3b",
   "metadata": {},
   "source": [
    "## Purchase Management [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "28a1d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/929338-purchase-management?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/929338-purchase-management/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Purchase Management' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3b3b",
   "metadata": {},
   "source": [
    "## Safety [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fb0c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/933812-safety/suggestions/44529945-sans-serif-fonts: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/933812-safety/suggestions/44529945-sans-serif-fonts (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5F0797070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/933812-safety/suggestions/44529945-sans-serif-fonts: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/933812-safety/suggestions/44529945-sans-serif-fonts (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5F079A070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/933812-safety/suggestions/41306257-adding-2fa-to-chat-as-optional-filter: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/933812-safety/suggestions/41306257-adding-2fa-to-chat-as-optional-filter (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5F2CB7220>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/933812-safety/suggestions/41306257-adding-2fa-to-chat-as-optional-filter: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/933812-safety/suggestions/41306257-adding-2fa-to-chat-as-optional-filter (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F5F2CB71F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/933812-safety?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/933812-safety/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Safety' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16df0c2",
   "metadata": {},
   "source": [
    "## Subscriptions [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dcfdc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310231-subscriptions/suggestions/38573176-add-higher-sub-gifting-badges: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310231-subscriptions/suggestions/38573176-add-higher-sub-gifting-badges (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F628962DC0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310231-subscriptions/suggestions/42603280-fair-payments-and-payouts: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310231-subscriptions/suggestions/42603280-fair-payments-and-payouts (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F62AEF1D00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310231-subscriptions/suggestions/42603280-fair-payments-and-payouts: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310231-subscriptions/suggestions/42603280-fair-payments-and-payouts (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F62AEF1CD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310231-subscriptions/suggestions/43444125-subscription-reminder-emails-lack-critical-informa: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310231-subscriptions/suggestions/43444125-subscription-reminder-emails-lack-critical-informa (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F63A85D0D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310231-subscriptions/suggestions/43444125-subscription-reminder-emails-lack-critical-informa: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310231-subscriptions/suggestions/43444125-subscription-reminder-emails-lack-critical-informa (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F63A85B0D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310231-subscriptions?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310231-subscriptions/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Subscriptions' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d72f3f",
   "metadata": {},
   "source": [
    "## Twitch Applications: Consoles [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a5e69b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310219-twitch-applications-consoles?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310219-twitch-applications-consoles/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Twitch Applications: Consoles' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5c5eb",
   "metadata": {},
   "source": [
    "## Twitch Applications: Mobile [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a7b42922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310222-twitch-applications-mobile/suggestions/39000541-swipe-to-remove-continue-watching-entry: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310222-twitch-applications-mobile/suggestions/39000541-swipe-to-remove-continue-watching-entry (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F66BA43430>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Error processing link https://twitch.uservoice.com/forums/310222-twitch-applications-mobile/suggestions/15724485-broadcasts-that-you-ve-recently-watched: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310222-twitch-applications-mobile/suggestions/15724485-broadcasts-that-you-ve-recently-watched (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F66BA43400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310222-twitch-applications-mobile?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310222-twitch-applications-mobile/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Twitch Applications: Mobile' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d873dc4",
   "metadata": {},
   "source": [
    "## Twitch Applications: TV Apps [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c714eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/310225-twitch-applications-tv-apps/suggestions/41734411-on-the-lg-webos-app-make-it-so-the-twitch-chat-do: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/310225-twitch-applications-tv-apps/suggestions/41734411-on-the-lg-webos-app-make-it-so-the-twitch-chat-do (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F6767210D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310225-twitch-applications-tv-apps?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310225-twitch-applications-tv-apps/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Twitch Applications: TV Apps' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41808540",
   "metadata": {},
   "source": [
    "## Twitch Studio [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b4d26a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/923041-twitch-studio/suggestions/38368711-build-in-audio-mixer-to-be-able-to-mute-programs-o: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/923041-twitch-studio/suggestions/38368711-build-in-audio-mixer-to-be-able-to-mute-programs-o (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F6A13367C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/923041-twitch-studio?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/923041-twitch-studio/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Twitch Studio' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a06e3",
   "metadata": {},
   "source": [
    "## User Accessibility [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f8729cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/926080-user-accessibility?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/926080-user-accessibility/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'User Accessibility' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7097cf6",
   "metadata": {},
   "source": [
    "## Video Features [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f23e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://twitch.uservoice.com/forums/923368-video-features/suggestions/43874217-remove-update-collection-limit: HTTPSConnectionPool(host='twitch.uservoice.comhttps', port=443): Max retries exceeded with url: //twitch.uservoice.com/forums/923368-video-features/suggestions/43874217-remove-update-collection-limit (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F6E32FB670>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/923368-video-features?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/923368-video-features/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Video Features' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ed1b3",
   "metadata": {},
   "source": [
    "## Video Performance [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6c4f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas.csv\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://twitch.uservoice.com/forums/310207-video-performance?filter=top&page=\"  ## Change this for other cat##\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "page_no = 1\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    url = base_url + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310207-video-performance/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]  ## Change this for other cat ##\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas): # Needed to seperate in order to have 2 requests\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            if t_tag == None:\n",
    "                topic = np.nan\n",
    "            else:\n",
    "                topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Video Performance' ##Change this for other cat ##\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            \n",
    "            # Had to add this because there were a number of faulty url's\n",
    "            try:\n",
    "                detailed_response = requests.get(detailed_url + link)\n",
    "                detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "                # Extract author and date\n",
    "                author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "                date = detailed_soup.find('time').get_text(strip=True)\n",
    "\n",
    "                # Write to the CSV\n",
    "                writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e539f",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "## Trial Scraper for chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3a705ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data is saved to twitch_ideas_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Target URL\n",
    "BASE_URL = \"https://twitch.uservoice.com/forums/310201-chat?filter=top&page=\"\n",
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "page_no = 1\n",
    "csv_file = 'twitch_ideas_test.csv'\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Name', 'Description', 'VoteCount', 'NumComments', 'Topic', 'Category', 'Date'])\n",
    "        \n",
    "# Iterate through each page\n",
    "while True:\n",
    "    if page_no > 2:# Stop after 15 pages\n",
    "        break\n",
    "    url = BASE_URL + str(page_no)\n",
    "    \n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all ideas\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # Find all detailed links\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310201-chat/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]\n",
    "    \n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idx, idea in enumerate(ideas):\n",
    "            \n",
    "            # Extract the required data\n",
    "            name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "            \n",
    "            # Had to add this check as some ideas dont include descriptions\n",
    "            description = idea.find('div', class_='typeset')\n",
    "            if description == None:\n",
    "                description = np.nan\n",
    "            else:\n",
    "                description = description.get_text(strip = True)\n",
    "            \n",
    "            vote_count = idea.find('div', class_='uvIdeaVoteCount').get_text(strip = True).replace(\"votes\",'').replace(\"vote\",'').replace(\",\",\"\").strip()\n",
    "            vote_count = int(vote_count)\n",
    "\n",
    "            # Extracting num_comments and topic, had to seperate one class\n",
    "            meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "\n",
    "            c_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "            num_comments = c_tag.get_text(strip=True).replace(\"comments\", \"\").replace(\"comment\",'').strip()\n",
    "            num_comments = int(num_comments)\n",
    "\n",
    "            t_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "            topic = t_tag.get_text(strip=True)\n",
    "            \n",
    "            # Initializing category for each scrape\n",
    "            category = 'Chat'\n",
    "            \n",
    "            # Grabbing detailed information\n",
    "            link = detailed_links[idx]\n",
    "            detailed_response = requests.get(detailed_url + link)\n",
    "            detailed_soup = BeautifulSoup(detailed_response.content, 'html.parser')\n",
    "\n",
    "            # Extract author and date\n",
    "            author = detailed_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "            date = detailed_soup.find('time').get_text(strip=True)\n",
    "            \n",
    "            # Write to the CSV\n",
    "            writer.writerow([author, name, description, vote_count, num_comments, topic, category, date])\n",
    "            \n",
    "        # Determining if there is a next page, if no page is found, break\n",
    "        next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "        if not next_page_indicator:\n",
    "            break\n",
    "\n",
    "        page_no += 1 \n",
    "print(\"Scraping completed and data is saved to twitch_ideas_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e663171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>VoteCount</th>\n",
       "      <th>NumComments</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iateyourpie</td>\n",
       "      <td>Keep Moments</td>\n",
       "      <td>Moments is a feature that Twitch released as a...</td>\n",
       "      <td>4191</td>\n",
       "      <td>453</td>\n",
       "      <td>Stream Chat</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2023-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iAndy88</td>\n",
       "      <td>Verified status for everyone and /verifiedonly...</td>\n",
       "      <td>I would appreciate a very helpful feature for ...</td>\n",
       "      <td>4049</td>\n",
       "      <td>246</td>\n",
       "      <td>Feature Request</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2021-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GohgoDude</td>\n",
       "      <td>Channel Points Leaderboard</td>\n",
       "      <td>Provide two leaderboards for channels that hav...</td>\n",
       "      <td>1883</td>\n",
       "      <td>21</td>\n",
       "      <td>Leaderboards</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Victor_sueca</td>\n",
       "      <td>/spoiler command</td>\n",
       "      <td>Users sending a spoiler could use this command...</td>\n",
       "      <td>817</td>\n",
       "      <td>15</td>\n",
       "      <td>Commands</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2016-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xSwagzy</td>\n",
       "      <td>Birthday Badge</td>\n",
       "      <td>It would be nice to have a birthday badge for ...</td>\n",
       "      <td>597</td>\n",
       "      <td>11</td>\n",
       "      <td>Feature Request</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2019-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LunarGabriel</td>\n",
       "      <td>Let chants be optional instead of removing them</td>\n",
       "      <td>I know Twitch says the feature seemed to cause...</td>\n",
       "      <td>573</td>\n",
       "      <td>42</td>\n",
       "      <td>Feature Request</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2022-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subwithpr1me</td>\n",
       "      <td>Ability to turn off chat reply/threads feature</td>\n",
       "      <td>The chat replies feature may be cool for some ...</td>\n",
       "      <td>513</td>\n",
       "      <td>41</td>\n",
       "      <td>Replies</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shiinto1993</td>\n",
       "      <td>Delete your own message</td>\n",
       "      <td>It would be very nice, if it's possible to del...</td>\n",
       "      <td>507</td>\n",
       "      <td>28</td>\n",
       "      <td>Stream Chat</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2019-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AdminTwitch(Admin, Twitch)</td>\n",
       "      <td>[Test] Crowd Chant</td>\n",
       "      <td>Some communities will notice a new way to \"cha...</td>\n",
       "      <td>441</td>\n",
       "      <td>259</td>\n",
       "      <td>Stream Chat</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2021-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S7ylehunter</td>\n",
       "      <td>Deleteable/Eraseble whispers or direct messages.</td>\n",
       "      <td>For give aways sometimes important.The message...</td>\n",
       "      <td>413</td>\n",
       "      <td>46</td>\n",
       "      <td>Whispers</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2023-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdminJon Bulava(Developer Relations, Twitch)</td>\n",
       "      <td>Developer Chat Badge</td>\n",
       "      <td>A chat badge to identify yourself in Twitch ch...</td>\n",
       "      <td>345</td>\n",
       "      <td>15</td>\n",
       "      <td>Stream Chat</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2019-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MoCoMade</td>\n",
       "      <td>Add a queue to /shoutout</td>\n",
       "      <td>This has been a great new feature that Twitch ...</td>\n",
       "      <td>336</td>\n",
       "      <td>35</td>\n",
       "      <td>Commands</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2022-11-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rafallero12</td>\n",
       "      <td>Recent chat history</td>\n",
       "      <td>As a few channels moderator, if i need to refr...</td>\n",
       "      <td>309</td>\n",
       "      <td>20</td>\n",
       "      <td>Feature Request</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ChefHazmat</td>\n",
       "      <td>Get Rid of Chat Replies</td>\n",
       "      <td>Completely get rid of the chat replies/thread ...</td>\n",
       "      <td>287</td>\n",
       "      <td>25</td>\n",
       "      <td>Replies</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ranozex</td>\n",
       "      <td>View twitch chat in VODS</td>\n",
       "      <td>Hello, I believe a better suggestion would be ...</td>\n",
       "      <td>215</td>\n",
       "      <td>6</td>\n",
       "      <td>Feature Request</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2015-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gozen_</td>\n",
       "      <td>Allow Incoming Viewers To See Previous Chat Me...</td>\n",
       "      <td>I would like to have the ability to enable an ...</td>\n",
       "      <td>210</td>\n",
       "      <td>4</td>\n",
       "      <td>Stream Chat</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MarTyNi</td>\n",
       "      <td>Option to not show the results before people vote</td>\n",
       "      <td>A lot of the times viewers will manipulate the...</td>\n",
       "      <td>205</td>\n",
       "      <td>4</td>\n",
       "      <td>Polls</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bluee_pc</td>\n",
       "      <td>Fix the constant disconnecting of chat.</td>\n",
       "      <td>Getting a constant \"Welcome to the chat room\" ...</td>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>Bugs</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vessi</td>\n",
       "      <td>Create a \"Global Paintbrush Badge\" for artists...</td>\n",
       "      <td>Artists who draw Twitch emotes have contribute...</td>\n",
       "      <td>181</td>\n",
       "      <td>9</td>\n",
       "      <td>Identity</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mellen</td>\n",
       "      <td>total follow time instead of follow date</td>\n",
       "      <td>ppl get triggered when they accidentally unfol...</td>\n",
       "      <td>166</td>\n",
       "      <td>71</td>\n",
       "      <td>User Card</td>\n",
       "      <td>Chat</td>\n",
       "      <td>2020-09-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Author  \\\n",
       "0                                    iateyourpie   \n",
       "1                                        iAndy88   \n",
       "2                                      GohgoDude   \n",
       "3                                   Victor_sueca   \n",
       "4                                        xSwagzy   \n",
       "5                                   LunarGabriel   \n",
       "6                                   subwithpr1me   \n",
       "7                                    shiinto1993   \n",
       "8                     AdminTwitch(Admin, Twitch)   \n",
       "9                                    S7ylehunter   \n",
       "10  AdminJon Bulava(Developer Relations, Twitch)   \n",
       "11                                      MoCoMade   \n",
       "12                                   rafallero12   \n",
       "13                                    ChefHazmat   \n",
       "14                                       ranozex   \n",
       "15                                        Gozen_   \n",
       "16                                       MarTyNi   \n",
       "17                                      bluee_pc   \n",
       "18                                         Vessi   \n",
       "19                                        mellen   \n",
       "\n",
       "                                                 Name  \\\n",
       "0                                        Keep Moments   \n",
       "1   Verified status for everyone and /verifiedonly...   \n",
       "2                          Channel Points Leaderboard   \n",
       "3                                    /spoiler command   \n",
       "4                                      Birthday Badge   \n",
       "5     Let chants be optional instead of removing them   \n",
       "6      Ability to turn off chat reply/threads feature   \n",
       "7                             Delete your own message   \n",
       "8                                  [Test] Crowd Chant   \n",
       "9    Deleteable/Eraseble whispers or direct messages.   \n",
       "10                               Developer Chat Badge   \n",
       "11                           Add a queue to /shoutout   \n",
       "12                                Recent chat history   \n",
       "13                            Get Rid of Chat Replies   \n",
       "14                           View twitch chat in VODS   \n",
       "15  Allow Incoming Viewers To See Previous Chat Me...   \n",
       "16  Option to not show the results before people vote   \n",
       "17            Fix the constant disconnecting of chat.   \n",
       "18  Create a \"Global Paintbrush Badge\" for artists...   \n",
       "19           total follow time instead of follow date   \n",
       "\n",
       "                                          Description  VoteCount  NumComments  \\\n",
       "0   Moments is a feature that Twitch released as a...       4191          453   \n",
       "1   I would appreciate a very helpful feature for ...       4049          246   \n",
       "2   Provide two leaderboards for channels that hav...       1883           21   \n",
       "3   Users sending a spoiler could use this command...        817           15   \n",
       "4   It would be nice to have a birthday badge for ...        597           11   \n",
       "5   I know Twitch says the feature seemed to cause...        573           42   \n",
       "6   The chat replies feature may be cool for some ...        513           41   \n",
       "7   It would be very nice, if it's possible to del...        507           28   \n",
       "8   Some communities will notice a new way to \"cha...        441          259   \n",
       "9   For give aways sometimes important.The message...        413           46   \n",
       "10  A chat badge to identify yourself in Twitch ch...        345           15   \n",
       "11  This has been a great new feature that Twitch ...        336           35   \n",
       "12  As a few channels moderator, if i need to refr...        309           20   \n",
       "13  Completely get rid of the chat replies/thread ...        287           25   \n",
       "14  Hello, I believe a better suggestion would be ...        215            6   \n",
       "15  I would like to have the ability to enable an ...        210            4   \n",
       "16  A lot of the times viewers will manipulate the...        205            4   \n",
       "17  Getting a constant \"Welcome to the chat room\" ...        185           45   \n",
       "18  Artists who draw Twitch emotes have contribute...        181            9   \n",
       "19  ppl get triggered when they accidentally unfol...        166           71   \n",
       "\n",
       "              Topic Category       Date  \n",
       "0       Stream Chat     Chat 2023-08-31  \n",
       "1   Feature Request     Chat 2021-01-12  \n",
       "2      Leaderboards     Chat 2020-01-08  \n",
       "3          Commands     Chat 2016-12-17  \n",
       "4   Feature Request     Chat 2019-11-27  \n",
       "5   Feature Request     Chat 2022-01-25  \n",
       "6           Replies     Chat 2020-08-10  \n",
       "7       Stream Chat     Chat 2019-10-08  \n",
       "8       Stream Chat     Chat 2021-05-18  \n",
       "9          Whispers     Chat 2023-10-01  \n",
       "10      Stream Chat     Chat 2019-10-29  \n",
       "11         Commands     Chat 2022-11-18  \n",
       "12  Feature Request     Chat 2018-12-03  \n",
       "13          Replies     Chat 2020-08-11  \n",
       "14  Feature Request     Chat 2015-05-29  \n",
       "15      Stream Chat     Chat 2020-05-05  \n",
       "16            Polls     Chat 2020-05-14  \n",
       "17             Bugs     Chat 2020-05-16  \n",
       "18         Identity     Chat 2017-03-26  \n",
       "19        User Card     Chat 2020-09-15  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('twitch_ideas.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086f640",
   "metadata": {},
   "source": [
    "## Sample CSV writer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23284da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://twitch.uservoice.com/forums/310201-chat?filter=top&page=\"\n",
    "page_no = 1\n",
    "csv_file = 'twitch_ideas.csv'\n",
    "\n",
    "# Check if the file exists. If not, write the header.\n",
    "if not os.path.exists(csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Name'])\n",
    "\n",
    "# Iterate through each page\n",
    "while True:\n",
    "    if page_no > 3:  # Stop after 3 pages\n",
    "        break\n",
    "\n",
    "    url = BASE_URL + str(page_no)\n",
    "\n",
    "    # Fetch website content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "\n",
    "    # Append data to the CSV file\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for idea in ideas:\n",
    "            # Extract the required data\n",
    "            name = idea.find('h2', class_='uvIdeaTitle uvCustomLink-color').get_text(strip=True)\n",
    "\n",
    "            # Write to the CSV\n",
    "            writer.writerow([name])\n",
    "\n",
    "    # Check for the next page. If no next page is found, break\n",
    "    next_page_indicator = soup.find('a', rel='next')\n",
    "    if not next_page_indicator:\n",
    "        print(\"Scraping completed and data is saved to twitch_ideas.csv\")\n",
    "        break\n",
    "\n",
    "    page_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260c732",
   "metadata": {},
   "source": [
    "## Testing data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "afaa1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_items = []\n",
    "for idea in ideas:\n",
    "    name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "    list_items.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dfbe7935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Keep Moments',\n",
       " 'Verified status for everyone and /verifiedonly chat mode',\n",
       " 'Channel Points Leaderboard',\n",
       " '/spoiler command',\n",
       " 'Birthday Badge',\n",
       " 'Let chants be optional instead of removing them',\n",
       " 'Ability to turn off chat reply/threads feature',\n",
       " 'Delete your own message',\n",
       " '[Test] Crowd Chant',\n",
       " 'Deleteable/Eraseble whispers or direct messages.',\n",
       " 'Developer Chat Badge',\n",
       " 'Add a queue to /shoutout',\n",
       " 'Recent chat history',\n",
       " 'Get Rid of Chat Replies',\n",
       " 'View twitch chat in VODS',\n",
       " 'Allow Incoming Viewers To See Previous Chat Messages',\n",
       " 'Option to not show the results before people vote',\n",
       " 'Fix the constant disconnecting of chat.',\n",
       " 'Create a \"Global Paintbrush Badge\" for artists who have designed emotes for a certain # of channels',\n",
       " 'total follow time instead of follow date']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3e0b2",
   "metadata": {},
   "source": [
    "## Seperating a single element into two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "edc3b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_items_1 = []\n",
    "list_items_2 = []\n",
    "for idea in ideas:\n",
    "    meta_div = idea.find('div', class_='uvIdeaMeta')\n",
    "        \n",
    "    comments_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Comments for\"))\n",
    "    num_comments = comments_tag.get_text(strip=True).replace(\"comments\", \"\").strip()\n",
    "    num_comments = int(num_comments)\n",
    "\n",
    "    category_tag = meta_div.find('a', title=lambda x: x and x.startswith(\"Ideas similar to\"))\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    list_items_1.append(num_comments)\n",
    "    list_items_2.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3f5a4bca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_items_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b0a5036b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_items_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be23708b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[453, 246, 21, 15, 11, 42, 41, 28, 259, 46, 15, 35, 20, 25, 6, 4, 3, 45, 9, 71]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_items_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b9a214a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stream Chat',\n",
       " 'Feature Request',\n",
       " 'Leaderboards',\n",
       " 'Commands',\n",
       " 'Feature Request',\n",
       " 'Feature Request',\n",
       " 'Replies',\n",
       " 'Stream Chat',\n",
       " 'Stream Chat',\n",
       " 'Whispers',\n",
       " 'Stream Chat',\n",
       " 'Commands',\n",
       " 'Feature Request',\n",
       " 'Replies',\n",
       " 'Feature Request',\n",
       " 'Stream Chat',\n",
       " 'Polls',\n",
       " 'Bugs',\n",
       " 'Identity',\n",
       " 'User Card']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_items_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f96b9",
   "metadata": {},
   "source": [
    "## Attempt at looping pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "088ec39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://twitch.uservoice.com/forums/310201-chat?filter=top&page=\"\n",
    "page_no = 1\n",
    "multi_test = []\n",
    "while True:\n",
    "    url = BASE_URL + str(page_no)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    \n",
    "    # All scraping logic here\n",
    "    for idea in ideas:\n",
    "        name = idea.find('h2',class_='uvIdeaTitle uvCustomLink-color').get_text(strip = True)\n",
    "        multi_test.append(name)\n",
    "\n",
    "    next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "    if not next_page_indicator:\n",
    "        break\n",
    "\n",
    "    page_no += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aa130360",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37adf763",
   "metadata": {},
   "source": [
    "## Attempt at grabbing detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fcc4acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detailed_url = \"https://twitch.uservoice.com\"\n",
    "BASE_URL = \"https://twitch.uservoice.com/forums/310201-chat?filter=top&page=\"\n",
    "page_no = 1\n",
    "authorl = []\n",
    "datel = []\n",
    "while True:\n",
    "    if page_no > 2:# Stop after 2 pages\n",
    "        break\n",
    "    url = BASE_URL + str(page_no)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    ideas = soup.find_all('li', class_='uvListItem uvIdea uvIdea-list')\n",
    "    detailed_links = [a['href'] for a in soup.find_all('a', href=True) if \"/forums/310201-chat/suggestions/\" in a['href'] and\n",
    "              \"#comments\" not in a['href']]\n",
    "    \n",
    "    # All scraping logic here\n",
    "    for link in detailed_links:\n",
    "        time.sleep(1)\n",
    "    \n",
    "        # Make a request to the detailed idea page\n",
    "        idea_response = requests.get(detailed_url + link)\n",
    "        idea_soup = BeautifulSoup(idea_response.content, 'html.parser')\n",
    "\n",
    "        # Extract author and date\n",
    "        author = idea_soup.find('span', class_='vcard').get_text(strip=True)\n",
    "        date = idea_soup.find('time').get_text(strip=True)\n",
    "        \n",
    "        authorl.append(author)\n",
    "        datel.append(date)\n",
    "\n",
    "    next_page_indicator = soup.find('a', rel='next')\n",
    "    \n",
    "    if not next_page_indicator:\n",
    "        break\n",
    "\n",
    "    page_no += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f615ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iateyourpie',\n",
       " 'iAndy88',\n",
       " 'GohgoDude',\n",
       " 'Victor_sueca',\n",
       " 'xSwagzy',\n",
       " 'LunarGabriel',\n",
       " 'subwithpr1me',\n",
       " 'shiinto1993',\n",
       " 'AdminTwitch(Admin, Twitch)',\n",
       " 'S7ylehunter',\n",
       " 'AdminJon Bulava(Developer Relations, Twitch)',\n",
       " 'MoCoMade',\n",
       " 'rafallero12',\n",
       " 'ChefHazmat',\n",
       " 'ranozex',\n",
       " 'Gozen_',\n",
       " 'MarTyNi',\n",
       " 'bluee_pc',\n",
       " 'Vessi',\n",
       " 'mellen',\n",
       " 'amygobrrr',\n",
       " 'Rzfff',\n",
       " 'J2xxR',\n",
       " 'ctrl_CH',\n",
       " 'RedWeird',\n",
       " 'James',\n",
       " 'ChiChi',\n",
       " 'Anonymous',\n",
       " 'Niuham',\n",
       " 'Anonymous',\n",
       " 'Mattie2171',\n",
       " 'damnzl',\n",
       " 'xmon390',\n",
       " 'Danat',\n",
       " 'sgtnoah',\n",
       " 'InvisibleMan__',\n",
       " 'treemarie',\n",
       " 'Obst',\n",
       " 'Wynced',\n",
       " 'andreams_tv']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b92ba2ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(authorl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "347c789b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'August 31, 2023'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "384dd81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datel[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
